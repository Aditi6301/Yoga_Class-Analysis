{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_crop (image):\n",
    "  height=image.shape[0]\n",
    "  width=image.shape[1]\n",
    "  h_lower=int(height/2+ (0.45*height))\n",
    "  h_upper=int(height/2- (0.38*height))\n",
    "  w_left=int(width/2-(0.3*width))\n",
    "  w_right=int(width/2+(0.2*width))\n",
    "  #print(h_upper)\n",
    "  image = image[h_upper:h_lower, w_left:w_right, :]\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yoga_vid_collected\\abhay_bhuj\n",
      "abhay_bhuj\n",
      "dataset\\Bhujangasana\\abhay_bhuj\n",
      "yoga_vid_collected\\abhay_padam\n",
      "abhay_padam\n",
      "dataset\\Padamasana\\abhay_padam\n",
      "yoga_vid_collected\\abhay_shav\n",
      "abhay_shav\n",
      "dataset\\Shavasana\\abhay_shav\n",
      "yoga_vid_collected\\abhay_tada\n",
      "abhay_tada\n",
      "dataset\\Tadasana\\abhay_tada\n",
      "yoga_vid_collected\\abhay_trik\n",
      "abhay_trik\n",
      "dataset\\Trikonasana\\abhay_trik\n",
      "yoga_vid_collected\\abhay_vriksh\n",
      "abhay_vriksh\n",
      "dataset\\Vrikshasana\\abhay_vriksh\n",
      "yoga_vid_collected\\ameya_bhuj\n",
      "ameya_bhuj\n",
      "dataset\\Bhujangasana\\ameya_bhuj\n",
      "yoga_vid_collected\\ameya_padam\n",
      "ameya_padam\n",
      "dataset\\Padamasana\\ameya_padam\n",
      "yoga_vid_collected\\ameya_shav\n",
      "ameya_shav\n",
      "dataset\\Shavasana\\ameya_shav\n",
      "yoga_vid_collected\\ameya_tada\n",
      "ameya_tada\n",
      "dataset\\Tadasana\\ameya_tada\n",
      "yoga_vid_collected\\ameya_trik\n",
      "ameya_trik\n",
      "dataset\\Trikonasana\\ameya_trik\n",
      "yoga_vid_collected\\ameya_vriksh\n",
      "ameya_vriksh\n",
      "dataset\\Vrikshasana\\ameya_vriksh\n",
      "yoga_vid_collected\\bhumi_bhuj\n",
      "bhumi_bhuj\n",
      "dataset\\Bhujangasana\\bhumi_bhuj\n",
      "yoga_vid_collected\\bhumi_padam\n",
      "bhumi_padam\n",
      "dataset\\Padamasana\\bhumi_padam\n",
      "yoga_vid_collected\\bhumi_shav\n",
      "bhumi_shav\n",
      "dataset\\Shavasana\\bhumi_shav\n",
      "yoga_vid_collected\\bhumi_tada\n",
      "bhumi_tada\n",
      "dataset\\Tadasana\\bhumi_tada\n",
      "yoga_vid_collected\\bhumi_trik\n",
      "bhumi_trik\n",
      "dataset\\Trikonasana\\bhumi_trik\n",
      "yoga_vid_collected\\bhumi_vriksh\n",
      "bhumi_vriksh\n",
      "dataset\\Vrikshasana\\bhumi_vriksh\n",
      "yoga_vid_collected\\deepa_bhuj\n",
      "deepa_bhuj\n",
      "dataset\\Bhujangasana\\deepa_bhuj\n",
      "yoga_vid_collected\\deepa_padam\n",
      "deepa_padam\n",
      "dataset\\Padamasana\\deepa_padam\n",
      "yoga_vid_collected\\deepa_shava\n",
      "deepa_shava\n",
      "dataset\\Shavasana\\deepa_shava\n",
      "yoga_vid_collected\\deepa_tada\n",
      "deepa_tada\n",
      "dataset\\Tadasana\\deepa_tada\n",
      "yoga_vid_collected\\deepa_trik\n",
      "deepa_trik\n",
      "dataset\\Trikonasana\\deepa_trik\n",
      "yoga_vid_collected\\deepa_vriksh\n",
      "deepa_vriksh\n",
      "dataset\\Vrikshasana\\deepa_vriksh\n",
      "yoga_vid_collected\\dristi_bhuj\n",
      "dristi_bhuj\n",
      "dataset\\Bhujangasana\\dristi_bhuj\n",
      "yoga_vid_collected\\dristi_padam\n",
      "dristi_padam\n",
      "dataset\\Padamasana\\dristi_padam\n",
      "yoga_vid_collected\\dristi_shav\n",
      "dristi_shav\n",
      "dataset\\Shavasana\\dristi_shav\n",
      "yoga_vid_collected\\dristi_tada\n",
      "dristi_tada\n",
      "dataset\\Tadasana\\dristi_tada\n",
      "yoga_vid_collected\\dristi_trik\n",
      "dristi_trik\n",
      "dataset\\Trikonasana\\dristi_trik\n",
      "yoga_vid_collected\\dristi_vriksh\n",
      "dristi_vriksh\n",
      "dataset\\Vrikshasana\\dristi_vriksh\n",
      "yoga_vid_collected\\harshav_bhuj\n",
      "harshav_bhuj\n",
      "dataset\\Bhujangasana\\harshav_bhuj\n",
      "yoga_vid_collected\\harshav_padam\n",
      "harshav_padam\n",
      "dataset\\Padamasana\\harshav_padam\n",
      "yoga_vid_collected\\harshav_shav\n",
      "harshav_shav\n",
      "dataset\\Shavasana\\harshav_shav\n",
      "yoga_vid_collected\\harshav_tada\n",
      "harshav_tada\n",
      "dataset\\Tadasana\\harshav_tada\n",
      "yoga_vid_collected\\harshav_trik\n",
      "harshav_trik\n",
      "dataset\\Trikonasana\\harshav_trik\n",
      "yoga_vid_collected\\harshav_vriksh\n",
      "harshav_vriksh\n",
      "dataset\\Vrikshasana\\harshav_vriksh\n",
      "yoga_vid_collected\\kaustuk_bhuj\n",
      "kaustuk_bhuj\n",
      "dataset\\Bhujangasana\\kaustuk_bhuj\n",
      "yoga_vid_collected\\kaustuk_padam\n",
      "kaustuk_padam\n",
      "dataset\\Padamasana\\kaustuk_padam\n",
      "yoga_vid_collected\\kaustuk_shav\n",
      "kaustuk_shav\n",
      "dataset\\Shavasana\\kaustuk_shav\n",
      "yoga_vid_collected\\kaustuk_tada\n",
      "kaustuk_tada\n",
      "dataset\\Tadasana\\kaustuk_tada\n",
      "yoga_vid_collected\\kaustuk_trik\n",
      "kaustuk_trik\n",
      "dataset\\Trikonasana\\kaustuk_trik\n",
      "yoga_vid_collected\\kaustuk_vriksh\n",
      "kaustuk_vriksh\n",
      "dataset\\Vrikshasana\\kaustuk_vriksh\n",
      "yoga_vid_collected\\lakshmi_bhuj\n",
      "lakshmi_bhuj\n",
      "dataset\\Bhujangasana\\lakshmi_bhuj\n",
      "yoga_vid_collected\\lakshmi_padam\n",
      "lakshmi_padam\n",
      "dataset\\Padamasana\\lakshmi_padam\n",
      "yoga_vid_collected\\lakshmi_shav\n",
      "lakshmi_shav\n",
      "dataset\\Shavasana\\lakshmi_shav\n",
      "yoga_vid_collected\\lakshmi_tada\n",
      "lakshmi_tada\n",
      "dataset\\Tadasana\\lakshmi_tada\n",
      "yoga_vid_collected\\lakshmi_vriksh\n",
      "lakshmi_vriksh\n",
      "dataset\\Vrikshasana\\lakshmi_vriksh\n",
      "yoga_vid_collected\\piyush_bhuj\n",
      "piyush_bhuj\n",
      "dataset\\Bhujangasana\\piyush_bhuj\n",
      "yoga_vid_collected\\piyush_padam\n",
      "piyush_padam\n",
      "dataset\\Padamasana\\piyush_padam\n",
      "yoga_vid_collected\\piyush_shav\n",
      "piyush_shav\n",
      "dataset\\Shavasana\\piyush_shav\n",
      "yoga_vid_collected\\piyush_tada\n",
      "piyush_tada\n",
      "dataset\\Tadasana\\piyush_tada\n",
      "yoga_vid_collected\\piyush_trik\n",
      "piyush_trik\n",
      "dataset\\Trikonasana\\piyush_trik\n",
      "yoga_vid_collected\\piyush_vriksh\n",
      "piyush_vriksh\n",
      "dataset\\Vrikshasana\\piyush_vriksh\n",
      "yoga_vid_collected\\pranshul_bhuj\n",
      "pranshul_bhuj\n",
      "dataset\\Bhujangasana\\pranshul_bhuj\n",
      "yoga_vid_collected\\pranshul_shav\n",
      "pranshul_shav\n",
      "dataset\\Shavasana\\pranshul_shav\n",
      "yoga_vid_collected\\pranshul_tada\n",
      "pranshul_tada\n",
      "dataset\\Tadasana\\pranshul_tada\n",
      "yoga_vid_collected\\pranshul_trik\n",
      "pranshul_trik\n",
      "dataset\\Trikonasana\\pranshul_trik\n",
      "yoga_vid_collected\\pranshul_vriksh\n",
      "pranshul_vriksh\n",
      "dataset\\Vrikshasana\\pranshul_vriksh\n",
      "yoga_vid_collected\\rakesh_bhuj\n",
      "rakesh_bhuj\n",
      "dataset\\Bhujangasana\\rakesh_bhuj\n",
      "yoga_vid_collected\\rakesh_padam\n",
      "rakesh_padam\n",
      "dataset\\Padamasana\\rakesh_padam\n",
      "yoga_vid_collected\\rakesh_shav\n",
      "rakesh_shav\n",
      "dataset\\Shavasana\\rakesh_shav\n",
      "yoga_vid_collected\\rakesh_tada\n",
      "rakesh_tada\n",
      "dataset\\Tadasana\\rakesh_tada\n",
      "yoga_vid_collected\\rakesh_trik\n",
      "rakesh_trik\n",
      "dataset\\Trikonasana\\rakesh_trik\n",
      "yoga_vid_collected\\rakesh_vriksh\n",
      "rakesh_vriksh\n",
      "dataset\\Vrikshasana\\rakesh_vriksh\n",
      "yoga_vid_collected\\santosh_bhuj\n",
      "santosh_bhuj\n",
      "dataset\\Bhujangasana\\santosh_bhuj\n",
      "yoga_vid_collected\\santosh_bhuj2\n",
      "santosh_bhuj2\n",
      "dataset\\Bhujangasana\\santosh_bhuj2\n",
      "yoga_vid_collected\\santosh_padam\n",
      "santosh_padam\n",
      "dataset\\Padamasana\\santosh_padam\n",
      "yoga_vid_collected\\santosh_shav\n",
      "santosh_shav\n",
      "dataset\\Shavasana\\santosh_shav\n",
      "yoga_vid_collected\\santosh_tada\n",
      "santosh_tada\n",
      "dataset\\Tadasana\\santosh_tada\n",
      "yoga_vid_collected\\santosh_trik\n",
      "santosh_trik\n",
      "dataset\\Trikonasana\\santosh_trik\n",
      "yoga_vid_collected\\santosh_vriksh\n",
      "santosh_vriksh\n",
      "dataset\\Vrikshasana\\santosh_vriksh\n",
      "yoga_vid_collected\\sarthak_bhuj\n",
      "sarthak_bhuj\n",
      "dataset\\Bhujangasana\\sarthak_bhuj\n",
      "yoga_vid_collected\\sarthak_padam\n",
      "sarthak_padam\n",
      "dataset\\Padamasana\\sarthak_padam\n",
      "yoga_vid_collected\\sarthak_tada\n",
      "sarthak_tada\n",
      "dataset\\Tadasana\\sarthak_tada\n",
      "yoga_vid_collected\\sarthak_trik\n",
      "sarthak_trik\n",
      "dataset\\Trikonasana\\sarthak_trik\n",
      "yoga_vid_collected\\sarthak_vriksh\n",
      "sarthak_vriksh\n",
      "dataset\\Vrikshasana\\sarthak_vriksh\n",
      "yoga_vid_collected\\sathak_shav\n",
      "sathak_shav\n",
      "dataset\\Shavasana\\sathak_shav\n",
      "yoga_vid_collected\\shiva_shav\n",
      "shiva_shav\n",
      "dataset\\Shavasana\\shiva_shav\n",
      "yoga_vid_collected\\shiva_tada\n",
      "shiva_tada\n",
      "dataset\\Tadasana\\shiva_tada\n",
      "yoga_vid_collected\\shiva_trik\n",
      "shiva_trik\n",
      "dataset\\Trikonasana\\shiva_trik\n",
      "yoga_vid_collected\\shiva_vriksh\n",
      "shiva_vriksh\n",
      "dataset\\Vrikshasana\\shiva_vriksh\n",
      "yoga_vid_collected\\shiv_bhuj\n",
      "shiv_bhuj\n",
      "dataset\\Bhujangasana\\shiv_bhuj\n",
      "yoga_vid_collected\\shiv_padam\n",
      "shiv_padam\n",
      "dataset\\Padamasana\\shiv_padam\n",
      "yoga_vid_collected\\veena_bhuj\n",
      "veena_bhuj\n",
      "dataset\\Bhujangasana\\veena_bhuj\n",
      "yoga_vid_collected\\veena_padma\n",
      "veena_padma\n",
      "dataset\\Padamasana\\veena_padma\n",
      "yoga_vid_collected\\veena_shav\n",
      "veena_shav\n",
      "dataset\\Shavasana\\veena_shav\n",
      "yoga_vid_collected\\veena_tada\n",
      "veena_tada\n",
      "dataset\\Tadasana\\veena_tada\n",
      "yoga_vid_collected\\veena_vriksh\n",
      "veena_vriksh\n",
      "dataset\\Vrikshasana\\veena_vriksh\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "def create_dir(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    except OSError:\n",
    "        print(f\"ERROR: creating directory with name {path}\")\n",
    "def save_frame(video_path, save_dir, gap=30):\n",
    "    name = video_path.split(\"/\")[-1].split(\".\")[0].lower()\n",
    "    subname=name[19:]\n",
    "    \n",
    "  \n",
    "  \n",
    "    print(name)\n",
    "    print(subname)\n",
    "    if (name.find(\"_bhu\") != -1) :\n",
    "          save_path = os.path.join(save_dir, \"Bhujangasana\",subname)  \n",
    "    elif (name.find(\"_padam\") != -1) or (name.find(\"_padma\") != -1):\n",
    "          save_path = os.path.join(save_dir, \"Padamasana\",subname)\n",
    "    elif (name.find(\"_shav\") != -1):\n",
    "          save_path = os.path.join(save_dir, \"Shavasana\",subname)\n",
    "    elif (name.find(\"_trik\") != -1):\n",
    "          save_path = os.path.join(save_dir, \"Trikonasana\",subname)\n",
    "    elif (name.find(\"_vriksh\") != -1):\n",
    "          save_path = os.path.join(save_dir, \"Vrikshasana\",subname)\n",
    "    elif (name.find(\"_tad\") != -1):\n",
    "        save_path = os.path.join(save_dir, \"Tadasana\",subname)\n",
    "    else:\n",
    "        print(subname)\n",
    "    print(save_path)\n",
    "    \n",
    "    create_dir(save_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    #fps = cap.get( cv2.CAP_PROP_FPS ) \n",
    "    #print(fps)\n",
    "    idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret == False:\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "        if (idx>=15 and (idx-gap//2) % gap == 0):\n",
    "            image=img_crop(frame)\n",
    "            cv2.imwrite(f\"{save_path}/{idx}.png\", image)\n",
    "\n",
    "        idx += 1\n",
    "if __name__ == \"__main__\":\n",
    "    video_paths = glob(\"Yoga_Vid_Collected/*\")\n",
    "    save_dir = \"dataset\"\n",
    "\n",
    "    for path in video_paths:\n",
    "        save_frame(path, save_dir, gap=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for visualization\n",
    "\n",
    "#@title Helper function for visualization\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n",
    "\n",
    "def to_gif(images, fps):\n",
    "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "  imageio.mimsave('./animation.gif', images, fps=fps)\n",
    "  return embed.embed_file('./animation.gif')\n",
    "\n",
    "def progress(value, max=100):\n",
    "  return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot select an axis to squeeze out which has size not equal to one",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1e15c2e73ff5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m display_image = tf.cast(tf.image.resize_with_pad(\n\u001b[0;32m      8\u001b[0m display_image, 1280, 1280), dtype=tf.int32)\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0moutput_overlay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw_prediction_on_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeypoints_tup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprocessed_image_array\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_overlay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[1;31m# keypoint_array = keypoint_with_scores ->\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msqueeze\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msqueeze\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m   1493\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1495\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
     ]
    }
   ],
   "source": [
    "# Visualize the predictions with image.\n",
    "\n",
    "\n",
    "image = tf.io.read_file('E:/BE/FinalYearProjectDetails/Yoga_Class-Analysis/dataset/Bhujangasana/abhay_bhuj/15.png')\n",
    "image = tf.image.decode_png(image)\n",
    "display_image = image\n",
    "display_image = tf.cast(tf.image.resize_with_pad(\n",
    "display_image, 1280, 1280), dtype=tf.int32)\n",
    "output_overlay = draw_prediction_on_image(np.squeeze(display_image.numpy(), axis=0), keypoints_tup[0][0])\n",
    "processed_image_array.append(output_overlay)\n",
    "  # keypoint_array = keypoint_with_scores ->\n",
    "\n",
    "  \n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "for i in range(len(processed_image_array)):\n",
    "  plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(processed_image_array[i])\n",
    "  plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image.\n",
    "def image_to_movenet(path):\n",
    "    \n",
    "    if (path.find(\"Bhujangasana\") != -1) :\n",
    "          label=\"Bhujangasana\"  \n",
    "    elif (path.find(\"Padamasana\") != -1):\n",
    "          label=\"Padamasana\"\n",
    "    elif (path.find(\"Shavasana\") != -1):\n",
    "          label=\"Shavasana\"\n",
    "    elif (path.find(\"Trikonasana\") != -1):\n",
    "          label=\"Trikonasana\"\n",
    "    elif (path.find(\"Vrikshasana\") != -1):\n",
    "          label=\"Vrikshasana\"\n",
    "    elif (path.find(\"Tadasana\") != -1):\n",
    "          label=\"Tadasana\"\n",
    "        \n",
    "    image_path = path\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.compat.v1.image.decode_jpeg(image)\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "    image = tf.cast(tf.image.resize_with_pad(image, 256, 256), dtype=tf.int32)\n",
    "\n",
    "    # Download the model from TF Hub.\n",
    "    model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "    movenet = model.signatures['serving_default']\n",
    "\n",
    "    # Run model inference.\n",
    "    outputs = movenet(image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints = outputs['output_0']\n",
    "    keypoints_tup=(keypoints,label)\n",
    "    keypts_labels.append(keypoints_tup)\n",
    "    print(path)\n",
    "    print(keypts_labels)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/BE/FinalYearProjectDetails/Yoga_Class-Analysis/dataset/Bhujangasana/abhay_bhuj/15.png\n",
      "[(<tf.Tensor: shape=(1, 1, 17, 3), dtype=float32, numpy=\n",
      "array([[[[0.78621894, 0.87021947, 0.50614953],\n",
      "         [0.7801042 , 0.87738633, 0.4593199 ],\n",
      "         [0.77937704, 0.87538415, 0.50933564],\n",
      "         [0.7547425 , 0.86347234, 0.5314946 ],\n",
      "         [0.75545913, 0.86047244, 0.5078999 ],\n",
      "         [0.7497095 , 0.8122004 , 0.5055896 ],\n",
      "         [0.7793317 , 0.81107634, 0.69365615],\n",
      "         [0.7556962 , 0.6882595 , 0.255724  ],\n",
      "         [0.7984366 , 0.6940263 , 0.56903315],\n",
      "         [0.78506917, 0.6002404 , 0.2885837 ],\n",
      "         [0.8078834 , 0.58478236, 0.49464354],\n",
      "         [0.73355955, 0.57866436, 0.6360941 ],\n",
      "         [0.74406546, 0.5759433 , 0.59036964],\n",
      "         [0.7438633 , 0.43917832, 0.64149356],\n",
      "         [0.77175796, 0.43098912, 0.42472148],\n",
      "         [0.7200045 , 0.33020777, 0.3187891 ],\n",
      "         [0.7472891 , 0.30311382, 0.38912213]]]], dtype=float32)>, 'Bhujangasana'), (<tf.Tensor: shape=(1, 1, 17, 3), dtype=float32, numpy=\n",
      "array([[[[0.7752273 , 0.87912154, 0.5893797 ],\n",
      "         [0.7615156 , 0.88218987, 0.6206248 ],\n",
      "         [0.7638898 , 0.880481  , 0.59206384],\n",
      "         [0.74586654, 0.85848904, 0.5556056 ],\n",
      "         [0.7511187 , 0.8573169 , 0.65656596],\n",
      "         [0.74968785, 0.8038533 , 0.57041734],\n",
      "         [0.7839574 , 0.803237  , 0.5592019 ],\n",
      "         [0.7541457 , 0.68630564, 0.35198042],\n",
      "         [0.78687453, 0.68783   , 0.493086  ],\n",
      "         [0.78530025, 0.6061284 , 0.39411536],\n",
      "         [0.8075648 , 0.5903452 , 0.57289326],\n",
      "         [0.72953904, 0.5716459 , 0.6657206 ],\n",
      "         [0.7422822 , 0.57109416, 0.58731484],\n",
      "         [0.74337554, 0.44285345, 0.5970736 ],\n",
      "         [0.77108294, 0.43309894, 0.45736915],\n",
      "         [0.7234162 , 0.3290228 , 0.36586517],\n",
      "         [0.7496727 , 0.30223578, 0.4763348 ]]]], dtype=float32)>, 'Bhujangasana'), (<tf.Tensor: shape=(1, 1, 17, 3), dtype=float32, numpy=\n",
      "array([[[[0.65791464, 0.8615759 , 0.46542707],\n",
      "         [0.6403765 , 0.85983056, 0.2793355 ],\n",
      "         [0.64234   , 0.85872763, 0.54230934],\n",
      "         [0.64424956, 0.82822734, 0.25648338],\n",
      "         [0.64882606, 0.8235554 , 0.742049  ],\n",
      "         [0.6637171 , 0.7666503 , 0.6146244 ],\n",
      "         [0.6844162 , 0.7744476 , 0.6225185 ],\n",
      "         [0.69224685, 0.68377215, 0.36968392],\n",
      "         [0.7217391 , 0.6856887 , 0.32083592],\n",
      "         [0.7291548 , 0.6884466 , 0.16091925],\n",
      "         [0.7901994 , 0.71407306, 0.16433915],\n",
      "         [0.7270101 , 0.57550037, 0.49442363],\n",
      "         [0.7418041 , 0.5790505 , 0.5480425 ],\n",
      "         [0.7306059 , 0.44716734, 0.51963097],\n",
      "         [0.7649825 , 0.43852967, 0.386969  ],\n",
      "         [0.7114508 , 0.3384789 , 0.24398744],\n",
      "         [0.74507207, 0.30224854, 0.47405893]]]], dtype=float32)>, 'Bhujangasana'), (<tf.Tensor: shape=(1, 1, 17, 3), dtype=float32, numpy=\n",
      "array([[[[0.54132265, 0.77455133, 0.5985128 ],\n",
      "         [0.52862275, 0.76944464, 0.18797803],\n",
      "         [0.53124213, 0.7663692 , 0.35213757],\n",
      "         [0.53488195, 0.73544174, 0.15867573],\n",
      "         [0.5422985 , 0.73210037, 0.5602766 ],\n",
      "         [0.5913943 , 0.6947869 , 0.52518505],\n",
      "         [0.6027291 , 0.7044656 , 0.59437555],\n",
      "         [0.7022276 , 0.6816055 , 0.6102737 ],\n",
      "         [0.7106003 , 0.692023  , 0.3689879 ],\n",
      "         [0.7901218 , 0.682101  , 0.23731402],\n",
      "         [0.81558937, 0.6921408 , 0.31312588],\n",
      "         [0.7280047 , 0.58369136, 0.48806393],\n",
      "         [0.73758084, 0.5865065 , 0.5878688 ],\n",
      "         [0.74445266, 0.44233143, 0.453413  ],\n",
      "         [0.7665231 , 0.44084698, 0.49201646],\n",
      "         [0.7279879 , 0.32124203, 0.25858316],\n",
      "         [0.75518066, 0.2830362 , 0.42073554]]]], dtype=float32)>, 'Bhujangasana'), (<tf.Tensor: shape=(1, 1, 17, 3), dtype=float32, numpy=\n",
      "array([[[[0.78621894, 0.87021947, 0.50614953],\n",
      "         [0.7801042 , 0.87738633, 0.4593199 ],\n",
      "         [0.77937704, 0.87538415, 0.50933564],\n",
      "         [0.7547425 , 0.86347234, 0.5314946 ],\n",
      "         [0.75545913, 0.86047244, 0.5078999 ],\n",
      "         [0.7497095 , 0.8122004 , 0.5055896 ],\n",
      "         [0.7793317 , 0.81107634, 0.69365615],\n",
      "         [0.7556962 , 0.6882595 , 0.255724  ],\n",
      "         [0.7984366 , 0.6940263 , 0.56903315],\n",
      "         [0.78506917, 0.6002404 , 0.2885837 ],\n",
      "         [0.8078834 , 0.58478236, 0.49464354],\n",
      "         [0.73355955, 0.57866436, 0.6360941 ],\n",
      "         [0.74406546, 0.5759433 , 0.59036964],\n",
      "         [0.7438633 , 0.43917832, 0.64149356],\n",
      "         [0.77175796, 0.43098912, 0.42472148],\n",
      "         [0.7200045 , 0.33020777, 0.3187891 ],\n",
      "         [0.7472891 , 0.30311382, 0.38912213]]]], dtype=float32)>, 'Bhujangasana'), (<tf.Tensor: shape=(1, 1, 17, 3), dtype=float32, numpy=\n",
      "array([[[[0.78621894, 0.87021947, 0.50614953],\n",
      "         [0.7801042 , 0.87738633, 0.4593199 ],\n",
      "         [0.77937704, 0.87538415, 0.50933564],\n",
      "         [0.7547425 , 0.86347234, 0.5314946 ],\n",
      "         [0.75545913, 0.86047244, 0.5078999 ],\n",
      "         [0.7497095 , 0.8122004 , 0.5055896 ],\n",
      "         [0.7793317 , 0.81107634, 0.69365615],\n",
      "         [0.7556962 , 0.6882595 , 0.255724  ],\n",
      "         [0.7984366 , 0.6940263 , 0.56903315],\n",
      "         [0.78506917, 0.6002404 , 0.2885837 ],\n",
      "         [0.8078834 , 0.58478236, 0.49464354],\n",
      "         [0.73355955, 0.57866436, 0.6360941 ],\n",
      "         [0.74406546, 0.5759433 , 0.59036964],\n",
      "         [0.7438633 , 0.43917832, 0.64149356],\n",
      "         [0.77175796, 0.43098912, 0.42472148],\n",
      "         [0.7200045 , 0.33020777, 0.3187891 ],\n",
      "         [0.7472891 , 0.30311382, 0.38912213]]]], dtype=float32)>, 'Bhujangasana')]\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import time\n",
    "\n",
    "# path_lst=[]\n",
    "# keypts_labels=[]\n",
    "# class_list=os.listdir('dataset')\n",
    "# for name in class_list:\n",
    "#     for image in os.listdir(f\"dataset/{name}\"):\n",
    "#         for item in os.listdir(f\"dataset/{name}/{image}\"):\n",
    "# #             add_tup=(os.path.abspath(f\"dataset/{name}/{image}/{item}\"),name)\n",
    "#             path_lst.append(os.path.abspath(f\"dataset/{name}/{image}/{item}\"))\n",
    "\n",
    "\n",
    "\n",
    "# path_lst = sorted(path_lst,key=os.path.getmtime)\n",
    "# for path in path_lst:\n",
    "#     image_to_movenet(path)\n",
    "path='E:/BE/FinalYearProjectDetails/Yoga_Class-Analysis/dataset/Bhujangasana/abhay_bhuj/15.png'\n",
    "image_to_movenet(path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
