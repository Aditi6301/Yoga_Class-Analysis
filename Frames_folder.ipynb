{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "# Import matplotlib libraries\n",
    "# from matplotlib import pyplot as plt\n",
    "# from matplotlib.collections import LineCollection\n",
    "# import matplotlib.patches as patches\n",
    "\n",
    "# import imageio\n",
    "# from IPython.display import HTML, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_crop (image):\n",
    "  height=image.shape[0]\n",
    "  width=image.shape[1]\n",
    "  h_lower=int(height/2+ (0.45*height))\n",
    "  h_upper=int(height/2- (0.38*height))\n",
    "  w_left=int(width/2-(0.3*width))\n",
    "  w_right=int(width/2+(0.2*width))\n",
    "  #print(h_upper)\n",
    "  image = image[h_upper:h_lower, w_left:w_right, :]\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\\abhay_bhuj\n",
      "abhay_bhuj\n",
      "dataset/train\\Bhujangasana\\abhay_bhuj\n",
      "train\\abhay_padam\n",
      "abhay_padam\n",
      "dataset/train\\Padamasana\\abhay_padam\n",
      "train\\abhay_shav\n",
      "abhay_shav\n",
      "dataset/train\\Shavasana\\abhay_shav\n",
      "train\\abhay_tada\n",
      "abhay_tada\n",
      "dataset/train\\Tadasana\\abhay_tada\n",
      "train\\abhay_trik\n",
      "abhay_trik\n",
      "dataset/train\\Trikonasana\\abhay_trik\n",
      "train\\abhay_vriksh\n",
      "abhay_vriksh\n",
      "dataset/train\\Vrikshasana\\abhay_vriksh\n",
      "train\\ameya_bhuj\n",
      "ameya_bhuj\n",
      "dataset/train\\Bhujangasana\\ameya_bhuj\n",
      "train\\ameya_padam\n",
      "ameya_padam\n",
      "dataset/train\\Padamasana\\ameya_padam\n",
      "train\\ameya_shav\n",
      "ameya_shav\n",
      "dataset/train\\Shavasana\\ameya_shav\n",
      "train\\ameya_tadasan\n",
      "ameya_tadasan\n",
      "dataset/train\\Tadasana\\ameya_tadasan\n",
      "train\\ameya_trikonasana\n",
      "ameya_trikonasana\n",
      "dataset/train\\Trikonasana\\ameya_trikonasana\n",
      "train\\ameya_vriksh\n",
      "ameya_vriksh\n",
      "dataset/train\\Vrikshasana\\ameya_vriksh\n",
      "train\\bhumi_bhuj\n",
      "bhumi_bhuj\n",
      "dataset/train\\Bhujangasana\\bhumi_bhuj\n",
      "train\\bhumi_padam\n",
      "bhumi_padam\n",
      "dataset/train\\Padamasana\\bhumi_padam\n",
      "train\\bhumi_shav\n",
      "bhumi_shav\n",
      "dataset/train\\Shavasana\\bhumi_shav\n",
      "train\\bhumi_tad\n",
      "bhumi_tad\n",
      "dataset/train\\Tadasana\\bhumi_tad\n",
      "train\\bhumi_trik\n",
      "bhumi_trik\n",
      "dataset/train\\Trikonasana\\bhumi_trik\n",
      "train\\bhumi_vriksh\n",
      "bhumi_vriksh\n",
      "dataset/train\\Vrikshasana\\bhumi_vriksh\n",
      "train\\deepa_bhujan\n",
      "deepa_bhujan\n",
      "dataset/train\\Bhujangasana\\deepa_bhujan\n",
      "train\\deepa_padmasan\n",
      "deepa_padmasan\n",
      "dataset/train\\Padamasana\\deepa_padmasan\n",
      "train\\deepa_shava\n",
      "deepa_shava\n",
      "dataset/train\\Shavasana\\deepa_shava\n",
      "train\\deepa_tadasan\n",
      "deepa_tadasan\n",
      "dataset/train\\Tadasana\\deepa_tadasan\n",
      "train\\deepa_trikon\n",
      "deepa_trikon\n",
      "dataset/train\\Trikonasana\\deepa_trikon\n",
      "train\\deepa_vriksh\n",
      "deepa_vriksh\n",
      "dataset/train\\Vrikshasana\\deepa_vriksh\n",
      "train\\dristi_bhuj\n",
      "dristi_bhuj\n",
      "dataset/train\\Bhujangasana\\dristi_bhuj\n",
      "train\\dristi_padam\n",
      "dristi_padam\n",
      "dataset/train\\Padamasana\\dristi_padam\n",
      "train\\dristi_shav\n",
      "dristi_shav\n",
      "dataset/train\\Shavasana\\dristi_shav\n",
      "train\\dristi_tadasan\n",
      "dristi_tadasan\n",
      "dataset/train\\Tadasana\\dristi_tadasan\n",
      "train\\dristi_trik\n",
      "dristi_trik\n",
      "dataset/train\\Trikonasana\\dristi_trik\n",
      "train\\dristi_vriksh\n",
      "dristi_vriksh\n",
      "dataset/train\\Vrikshasana\\dristi_vriksh\n",
      "train\\harshav_bhuj\n",
      "harshav_bhuj\n",
      "dataset/train\\Bhujangasana\\harshav_bhuj\n",
      "train\\harshav_padam\n",
      "harshav_padam\n",
      "dataset/train\\Padamasana\\harshav_padam\n",
      "train\\harshav_shav\n",
      "harshav_shav\n",
      "dataset/train\\Shavasana\\harshav_shav\n",
      "train\\harshav_tad\n",
      "harshav_tad\n",
      "dataset/train\\Tadasana\\harshav_tad\n",
      "train\\harshav_trikon\n",
      "harshav_trikon\n",
      "dataset/train\\Trikonasana\\harshav_trikon\n",
      "train\\harshav_vriksh\n",
      "harshav_vriksh\n",
      "dataset/train\\Vrikshasana\\harshav_vriksh\n",
      "train\\kaustuk_bhuj\n",
      "kaustuk_bhuj\n",
      "dataset/train\\Bhujangasana\\kaustuk_bhuj\n",
      "train\\kaustuk_padam\n",
      "kaustuk_padam\n",
      "dataset/train\\Padamasana\\kaustuk_padam\n",
      "train\\kaustuk_shav\n",
      "kaustuk_shav\n",
      "dataset/train\\Shavasana\\kaustuk_shav\n",
      "train\\kaustuk_tadasan\n",
      "kaustuk_tadasan\n",
      "dataset/train\\Tadasana\\kaustuk_tadasan\n",
      "train\\kaustuk_trik\n",
      "kaustuk_trik\n",
      "dataset/train\\Trikonasana\\kaustuk_trik\n",
      "train\\kaustuk_vriksh\n",
      "kaustuk_vriksh\n",
      "dataset/train\\Vrikshasana\\kaustuk_vriksh\n",
      "train\\lakshmi_bhujang\n",
      "lakshmi_bhujang\n",
      "dataset/train\\Bhujangasana\\lakshmi_bhujang\n",
      "train\\lakshmi_padam\n",
      "lakshmi_padam\n",
      "dataset/train\\Padamasana\\lakshmi_padam\n",
      "train\\lakshmi_shavasan\n",
      "lakshmi_shavasan\n",
      "dataset/train\\Shavasana\\lakshmi_shavasan\n",
      "train\\lakshmi_tadasan\n",
      "lakshmi_tadasan\n",
      "dataset/train\\Tadasana\\lakshmi_tadasan\n",
      "train\\lakshmi_vriksh\n",
      "lakshmi_vriksh\n",
      "dataset/train\\Vrikshasana\\lakshmi_vriksh\n",
      "train\\piyush_bhuj\n",
      "piyush_bhuj\n",
      "dataset/train\\Bhujangasana\\piyush_bhuj\n",
      "train\\piyush_padam\n",
      "piyush_padam\n",
      "dataset/train\\Padamasana\\piyush_padam\n",
      "train\\piyush_shav\n",
      "piyush_shav\n",
      "dataset/train\\Shavasana\\piyush_shav\n",
      "train\\piyush_tad\n",
      "piyush_tad\n",
      "dataset/train\\Tadasana\\piyush_tad\n",
      "train\\piyush_trikon\n",
      "piyush_trikon\n",
      "dataset/train\\Trikonasana\\piyush_trikon\n",
      "train\\piyush_vriksh\n",
      "piyush_vriksh\n",
      "dataset/train\\Vrikshasana\\piyush_vriksh\n",
      "train\\pranshul_bhuj\n",
      "pranshul_bhuj\n",
      "dataset/train\\Bhujangasana\\pranshul_bhuj\n",
      "train\\pranshul_shav\n",
      "pranshul_shav\n",
      "dataset/train\\Shavasana\\pranshul_shav\n",
      "train\\pranshul_tad\n",
      "pranshul_tad\n",
      "dataset/train\\Tadasana\\pranshul_tad\n",
      "train\\pranshul_trik\n",
      "pranshul_trik\n",
      "dataset/train\\Trikonasana\\pranshul_trik\n",
      "train\\pranshul_vriksh\n",
      "pranshul_vriksh\n",
      "dataset/train\\Vrikshasana\\pranshul_vriksh\n",
      "train\\rakesh_bhuj\n",
      "rakesh_bhuj\n",
      "dataset/train\\Bhujangasana\\rakesh_bhuj\n",
      "train\\rakesh_padam\n",
      "rakesh_padam\n",
      "dataset/train\\Padamasana\\rakesh_padam\n",
      "train\\rakesh_shav\n",
      "rakesh_shav\n",
      "dataset/train\\Shavasana\\rakesh_shav\n",
      "train\\rakesh_tadasna\n",
      "rakesh_tadasna\n",
      "dataset/train\\Tadasana\\rakesh_tadasna\n",
      "train\\rakesh_trik\n",
      "rakesh_trik\n",
      "dataset/train\\Trikonasana\\rakesh_trik\n",
      "train\\rakesh_vriksh\n",
      "rakesh_vriksh\n",
      "dataset/train\\Vrikshasana\\rakesh_vriksh\n",
      "train\\santosh_bhuj\n",
      "santosh_bhuj\n",
      "dataset/train\\Bhujangasana\\santosh_bhuj\n",
      "train\\santosh_bhuj2\n",
      "santosh_bhuj2\n",
      "dataset/train\\Bhujangasana\\santosh_bhuj2\n",
      "train\\santosh_padam\n",
      "santosh_padam\n",
      "dataset/train\\Padamasana\\santosh_padam\n",
      "train\\santosh_shav\n",
      "santosh_shav\n",
      "dataset/train\\Shavasana\\santosh_shav\n",
      "train\\santosh_tada\n",
      "santosh_tada\n",
      "dataset/train\\Tadasana\\santosh_tada\n",
      "train\\santosh_trik\n",
      "santosh_trik\n",
      "dataset/train\\Trikonasana\\santosh_trik\n",
      "train\\santosh_vriksh\n",
      "santosh_vriksh\n",
      "dataset/train\\Vrikshasana\\santosh_vriksh\n",
      "test\\sarthak_bhuj\n",
      "arthak_bhuj\n",
      "dataset/test\\Bhujangasana\\arthak_bhuj\n",
      "test\\sarthak_padam\n",
      "arthak_padam\n",
      "dataset/test\\Padamasana\\arthak_padam\n",
      "test\\sarthak_tada\n",
      "arthak_tada\n",
      "dataset/test\\Tadasana\\arthak_tada\n",
      "test\\sarthak_trik\n",
      "arthak_trik\n",
      "dataset/test\\Trikonasana\\arthak_trik\n",
      "test\\sarthak_vriksh\n",
      "arthak_vriksh\n",
      "dataset/test\\Vrikshasana\\arthak_vriksh\n",
      "test\\sathak_shav\n",
      "athak_shav\n",
      "dataset/test\\Shavasana\\athak_shav\n",
      "test\\shiva_shavasana\n",
      "hiva_shavasana\n",
      "dataset/test\\Shavasana\\hiva_shavasana\n",
      "test\\shiva_tadasana\n",
      "hiva_tadasana\n",
      "dataset/test\\Tadasana\\hiva_tadasana\n",
      "test\\shiva_trik\n",
      "hiva_trik\n",
      "dataset/test\\Trikonasana\\hiva_trik\n",
      "test\\shiva_vriksh\n",
      "hiva_vriksh\n",
      "dataset/test\\Vrikshasana\\hiva_vriksh\n",
      "test\\shiv_bhuj\n",
      "hiv_bhuj\n",
      "dataset/test\\Bhujangasana\\hiv_bhuj\n",
      "test\\shiv_padam\n",
      "hiv_padam\n",
      "dataset/test\\Padamasana\\hiv_padam\n",
      "test\\veena_bhu\n",
      "eena_bhu\n",
      "dataset/test\\Bhujangasana\\eena_bhu\n",
      "test\\veena_padmasan\n",
      "eena_padmasan\n",
      "dataset/test\\Padamasana\\eena_padmasan\n",
      "test\\veena_shav\n",
      "eena_shav\n",
      "dataset/test\\Shavasana\\eena_shav\n",
      "test\\veena_tadasna\n",
      "eena_tadasna\n",
      "dataset/test\\Tadasana\\eena_tadasna\n",
      "test\\veena_vriksh\n",
      "eena_vriksh\n",
      "dataset/test\\Vrikshasana\\eena_vriksh\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "def create_dir(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    except OSError:\n",
    "        print(f\"ERROR: creating directory with name {path}\")\n",
    "def save_frame(video_path, save_dir, gap=30):\n",
    "    name = video_path.split(\"/\")[-1].split(\".\")[0].lower()\n",
    "    subname=name[6:]\n",
    "    \n",
    "  \n",
    "  \n",
    "    print(name)\n",
    "    print(subname)\n",
    "    if (name.find(\"_bhu\") != -1) :\n",
    "          save_path = os.path.join(save_dir, \"Bhujangasana\",subname)  \n",
    "    elif (name.find(\"_padam\") != -1) or (name.find(\"_padma\") != -1):\n",
    "          save_path = os.path.join(save_dir, \"Padamasana\",subname)\n",
    "    elif (name.find(\"_shav\") != -1):\n",
    "          save_path = os.path.join(save_dir, \"Shavasana\",subname)\n",
    "    elif (name.find(\"_trik\") != -1):\n",
    "          save_path = os.path.join(save_dir, \"Trikonasana\",subname)\n",
    "    elif (name.find(\"_vriksh\") != -1):\n",
    "          save_path = os.path.join(save_dir, \"Vrikshasana\",subname)\n",
    "    elif (name.find(\"_tad\") != -1):\n",
    "        save_path = os.path.join(save_dir, \"Tadasana\",subname)\n",
    "    else:\n",
    "        print(subname)\n",
    "    print(save_path)\n",
    "    \n",
    "    create_dir(save_path)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    #fps = cap.get( cv2.CAP_PROP_FPS ) \n",
    "    #print(fps)\n",
    "    idx = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret == False:\n",
    "            cap.release()\n",
    "            break\n",
    "\n",
    "        if (idx>=15 and (idx-gap//2) % gap == 0):\n",
    "            image=img_crop(frame)\n",
    "            cv2.imwrite(f\"{save_path}/{idx}.png\", image)\n",
    "\n",
    "        idx += 1\n",
    "if __name__ == \"__main__\":\n",
    "    video_paths = glob(\"Yoga_Vid_Collected/train/*\")\n",
    "    save_dir = \"dataset/train\"\n",
    "    \n",
    "\n",
    "    for path in video_paths:\n",
    "        save_frame(path, save_dir, gap=30)\n",
    "    \n",
    "    video_paths = glob(\"Yoga_Vid_Collected/test/*\")\n",
    "    save_dir = \"dataset/test\"\n",
    "    \n",
    "    for path in video_paths:\n",
    "        save_frame(path, save_dir, gap=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypts_abs(keypoints):\n",
    "  kpts_x = keypoints[0, 0, :, 0]\n",
    "  kpts_y = keypoints[0, 0, :, 1]\n",
    "  kpts_scores = keypoints[0, 0, :, 2]\n",
    "\n",
    "  print(kpts_x.shape)\n",
    "  kpts_absolute_xy = np.stack(\n",
    "      [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "\n",
    "  kpts_absolute_xy.shape\n",
    "\n",
    "  keypoint_threshold=0.2\n",
    "  kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "      kpts_scores > keypoint_threshold, :]\n",
    "  return kpts_above_thresh_absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "# KEYPOINT_DICT = [\n",
    "#     'nose',\n",
    "#     'left_eye',\n",
    "#     'right_eye',\n",
    "#     'left_ear',\n",
    "#     'right_ear',\n",
    "#     'left_shoulder',\n",
    "#     'right_shoulder',\n",
    "#     'left_elbow',\n",
    "#     'right_elbow',\n",
    "#     'left_wrist',\n",
    "#     'right_wrist',\n",
    "#     'left_hip',\n",
    "#     'right_hip',\n",
    "#     'left_knee',\n",
    "#     'right_knee',\n",
    "#     'left_ankle',\n",
    "#     'right_ankle'\n",
    "# ]\n",
    "# for p in KEYPOINT_DICT:\n",
    "        \n",
    "#         data.append(p + \"_x\")\n",
    "#         data.append(p + \"_y\")\n",
    "#         data.append(p + \"_vis\")\n",
    "# data.append( \"_label\")\n",
    "data = pd.DataFrame(columns = data) # Empty dataset\n",
    "print(data)\n",
    "data.to_csv(\"dataset3.csv\") # save the data as a csv file\n",
    "# Load the input image.\n",
    "def image_to_movenet(path,keypts_labels):\n",
    "    \n",
    "    if (path.find(\"Bhujangasana\") != -1) :\n",
    "          label=\"Bhujangasana\"  \n",
    "    elif (path.find(\"Padamasana\") != -1):\n",
    "          label=\"Padamasana\"\n",
    "    elif (path.find(\"Shavasana\") != -1):\n",
    "          label=\"Shavasana\"\n",
    "    elif (path.find(\"Trikonasana\") != -1):\n",
    "          label=\"Trikonasana\"\n",
    "    elif (path.find(\"Vrikshasana\") != -1):\n",
    "          label=\"Vrikshasana\"\n",
    "    elif (path.find(\"Tadasana\") != -1):\n",
    "          label=\"Tadasana\"\n",
    "        \n",
    "    image_path = path\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.compat.v1.image.decode_jpeg(image)\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "    image = tf.cast(tf.image.resize_with_pad(image, 256, 256), dtype=tf.int32)\n",
    "\n",
    "    # Download the model from TF Hub.\n",
    "    model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "    movenet = model.signatures['serving_default']\n",
    "\n",
    "    # Run model inference.\n",
    "    outputs = movenet(image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints = outputs['output_0']\n",
    "    kpts_x = keypoints[0, 0, :, 0]\n",
    "    kpts_y = keypoints[0, 0, :, 1]\n",
    "    kpts_scores = keypoints[0, 0, :, 2]\n",
    "    \n",
    "    keypoints_tup=(keypoints,label)\n",
    "    keypts_labels.append(keypoints_tup)\n",
    "    with open('dataset3.csv','w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['nose_x', 'nose_y', 'nose_vis', 'left_eye_x', 'left_eye_y', 'left_eye_vis', 'right_eye_x', 'right_eye_y', 'right_eye_vis', 'left_ear_x', 'left_ear_y', 'left_ear_vis', 'right_ear_x', 'right_ear_y', 'right_ear_vis', 'left_shoulder_x', 'left_shoulder_y', 'left_shoulder_vis', 'right_shoulder_x', 'right_shoulder_y', 'right_shoulder_vis', 'left_elbow_x', 'left_elbow_y', 'left_elbow_vis', 'right_elbow_x', 'right_elbow_y', 'right_elbow_vis', 'left_wrist_x', 'left_wrist_y', 'left_wrist_vis', 'right_wrist_x', 'right_wrist_y', 'right_wrist_vis', 'left_hip_x', 'left_hip_y', 'left_hip_vis', 'right_hip_x', 'right_hip_y', 'right_hip_vis', 'left_knee_x', 'left_knee_y', 'left_knee_vis', 'right_knee_x', 'right_knee_y', 'right_knee_vis', 'left_ankle_x', 'left_ankle_y', 'left_ankle_vis', 'right_ankle_x', 'right_ankle_y', 'right_ankle_vis', 'label'])\n",
    "        for i in range(0,17):\n",
    "            csv_out.writerows(kpts_x[i])\n",
    "            csv_out.writerows(kpts_y[i])\n",
    "            csv_out.writerows(kpts_scores[i])\n",
    "        csv_out.writerows(label)  \n",
    "#    \n",
    "#     print(path)\n",
    "#     print(keypts_labels)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot iterate over a scalar tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-e39bdf49c344>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mpath_lst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetmtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpath_lst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mimage_to_movenet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeypts_labels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# class_list=os.listdir('dataset/test')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-b71b6e05f894>\u001b[0m in \u001b[0;36mimage_to_movenet\u001b[1;34m(path, keypts_labels)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mcsv_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nose_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nose_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nose_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_eye_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_eye_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_eye_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_eye_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_eye_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_eye_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_ear_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_ear_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_ear_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_ear_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_ear_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_ear_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_shoulder_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_shoulder_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_shoulder_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_shoulder_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_shoulder_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_shoulder_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_elbow_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_elbow_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_elbow_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_elbow_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_elbow_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_elbow_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_wrist_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_wrist_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_wrist_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_wrist_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_wrist_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_wrist_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_hip_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_hip_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_hip_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_hip_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_hip_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_hip_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_knee_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_knee_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_knee_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_knee_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_knee_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_knee_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_ankle_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_ankle_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left_ankle_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_ankle_x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_ankle_y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'right_ankle_vis'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mcsv_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkpts_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mcsv_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkpts_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mcsv_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkpts_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\radha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    524\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot iterate over a tensor with unknown shape.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot iterate over a scalar tensor.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m       raise TypeError(\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot iterate over a scalar tensor."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "path_lst=[]\n",
    "keypts_labels_train=[]\n",
    "keypts_labels_test=[]\n",
    "\n",
    "class_list=os.listdir('dataset/train')\n",
    "for name in class_list:\n",
    "    for image in os.listdir(f\"dataset/train/{name}\"):\n",
    "        for item in os.listdir(f\"dataset/train/{name}/{image}\"):\n",
    "             add_tup=(os.path.abspath(f\"dataset/train/{name}/{image}/{item}\"),name)\n",
    "             path_lst.append(os.path.abspath(f\"dataset/train/{name}/{image}/{item}\"))\n",
    "\n",
    "\n",
    "\n",
    "path_lst = sorted(path_lst,key=os.path.getmtime)\n",
    "for path in path_lst:\n",
    "    image_to_movenet(path,keypts_labels_train)\n",
    "\n",
    "# class_list=os.listdir('dataset/test')\n",
    "# for name in class_list:\n",
    "#     for image in os.listdir(f\"dataset/test/{name}\"):\n",
    "#         for item in os.listdir(f\"dataset/test/{name}/{image}\"):\n",
    "#              add_tup=(os.path.abspath(f\"dataset/test/{name}/{image}/{item}\"),name)\n",
    "#              path_lst.append(os.path.abspath(f\"dataset/test/{name}/{image}/{item}\"))\n",
    "\n",
    "\n",
    "\n",
    "# path_lst = sorted(path_lst,key=os.path.getmtime)\n",
    "# for path in path_lst:\n",
    "#     image_to_movenet(path,keypts_labels_test)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
